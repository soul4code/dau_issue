Fast DAU & 7‑Day Retention from a Large Events Table**   
**Сценарий:** Нужно посчитать DAU по дням и количество вернувшихся пользователей за последние 7 дней, забирая данные из большой таблице events.  
**Реализация:**

* Схема: events(user\_id int, course\_id int, kind text, performed\_at timestamp, properties jsonb).  
  * Напишите SQL (или используйте Django ORM) для получения:  
    * Список DAU по дням на выбранном диапазоне дат.  
    * Список пользователей, активных в день D и также активных в D‑1..D‑7.  
  * Предложите конкретные индексы.  
  * Необязательно: наметьте небольшую **rollup**‑таблицу для снижения затрат.  
* **Критерии выполнения (должно быть показано в тестах):**  
  * Корректные результаты на синтетическом наборе данных, созданном вами.  
  * Схема индексов в DDL соответствует реальным паттернам запросов и предотвращает full scan.  
  * Необязательно: определите, как балансировать свежесть данных и стоимость обработки (например, daily rollup \+ hourly delta).


**Установка**

1. Настроить окружение с python==3.7.17
2. Установить poetry `pip install poetry`
3. Установить зависимости `poetry install`
4. Запустить `pre-commit install`
5. Создать базу данных на сервере postrges и создать файл `.env` с настройками по подключению к БД в корне проекта по примеру `env.example`
6. Запустить миграции `python manage.py migrate`

**Генерация данных**

Для генерации данных написана management команда `create_events`. Генерирует данные случайным образом.

**Запуск тестов**

Для запуска тестов выполнить `pytest` в корне проекта

**Комментарии к выполненному заданию**

1. Не уверен что все правильно понял в задании по поводу вернувшихся пользователей. При реальной задаче конечно бы уточнил, но сейчас решил не заморачиваться и сделать как понял. Комментарии к запросам отписал.
2. Пришлось создать кастомный индекс на преобразованнный в дату performed_at. В задании не указано нужно ли было заморачиваться с часовыми поясами, но учитывая что django по умочанию их включает, оставил с ними
3. Наметил Celery таску которая раз в час заполняет DAU rollup таблицу за вчера и сегодня
4. Проверил на 400000 записей через Explain Analyse, индексы работают как надо, результаты положил в event/__tests__/results
5. Админку и апи не делал, предположил что в рамках данной задачи не очень важно
